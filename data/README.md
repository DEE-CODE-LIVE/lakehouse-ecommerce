# ðŸ›’ Lakehouse Ecommerce Project

â†’ This project demonstrates a complete end-to-end Data Engineering pipeline built using the Lakehouse architecture.

â†’ Starting from a raw ecommerce dataset downloaded from Kaggle, the data is ingested into Amazon S3, crawled with AWS Glue, and queried through Amazon Athena.

â†’ Transformations are performed using dbt, where data is modeled into staging, dimension, and fact tables following a star schema.

â†’ Tests and snapshots are implemented to ensure data quality and track changes.

â†’ Insights such as top sales by country and top customers by revenue are visualized using Amazon QuickSight.

â†’ The entire project is version-controlled and documented on GitHub.

---

## ðŸ”— Pipeline Overview

â†’ **Kaggle** (raw dataset)  
â†’ **Amazon S3** (data lake storage)  
â†’ **AWS Glue Crawler** (schema inference)  
â†’ **Amazon Athena** (SQL querying)  
â†’ **dbt** (transformations, tests, star schema)  
â†’ **Amazon QuickSight** (dashboards)  
â†’ **Git + GitHub** (version control)

---

## ðŸ§± Folder Structure

```bash
lakehouse_project1/
â”œâ”€â”€ analyses/              # dbt analysis files
â”œâ”€â”€ macros/                # dbt macros
â”œâ”€â”€ models/                # staging, dimensions, facts
â”œâ”€â”€ seeds/                 # static reference CSVs
â”œâ”€â”€ snapshots/             # historical snapshots
â”œâ”€â”€ tests/                 # dbt tests
â”œâ”€â”€ dbt_project.yml        # dbt config
â”œâ”€â”€ .gitignore             
â”œâ”€â”€ README.md
data/
â””â”€â”€ ecommerce_orders.csv   # raw dataset from Kaggle

logs/
â”œâ”€â”€ dbt.log
quicksight-img1.png    # dashboard screenshot
